// This file contains the string templates for the generated Motia project files.

export const PACKAGE_JSON = (projectName) => JSON.stringify({
  name: projectName,
 description: "",
  type: "module",
  scripts: {
    "postinstall": "motia install",
    "dev": "motia dev",
    "start": "motia start",
    "generate-types": "motia generate-types",
    "build": "motia build",
    "clean": "rm -rf dist node_modules python_modules .motia .mermaid"
  },
  keywords: [
    "motia"
  ],
  dependencies: {
    "@huggingface/inference": "^4.13.5",
    "@langchain/community": "1.1.1",
    "@langchain/core": "1.1.5",
    "@langchain/google-genai": "2.1.0",
    "@motiadev/adapter-bullmq-events": "^0.17.9-beta.191",
    "@motiadev/core": "^0.17.9-beta.191",
    "@motiadev/plugin-bullmq": "^0.17.9-beta.191",
    "@motiadev/plugin-endpoint": "^0.17.9-beta.191",
    "@motiadev/plugin-logs": "^0.17.9-beta.191",
    "@motiadev/plugin-observability": "^0.17.9-beta.191",
    "@motiadev/plugin-states": "^0.17.9-beta.191",
    "langchain": "1.2.0",
    "motia": "^0.17.9-beta.191"
  },
  devDependencies: {
    "@motiadev/workbench": "^0.17.9-beta.191",
    "@types/react": "^19.1.1",
    "ts-node": "^10.9.2",
    "typescript": "^5.7.3"
  }
}, null, 2);

export const ENV_EXAMPLE = `# Nebius AI configuration TODO: replace with your own values
NEBIUS_API_KEY=your_key_here
NEBIUS_BASE_URL=https://api.studio.nebius.com/v1/
NEBIUS_MODEL=openai/gpt-oss-20b
`;

export const README = (projectName) => `# ${projectName}

Generated by Motia Crew Generator. This project sets up an autonomous agent crew orchestrated by a MasterAgent using Nebius LLM service.

You just have to make changes where ever TODO comments are present and run the project!

## Next Steps

1. \`npm install\`
2. Change information or customize your logics where ever TODO comments are present.
    2.1 \`.env.example\`
    2.2 \`start_pipeline_api_step.js\`
    2.3 \`master_agent_step.js\`
    2.4 \`src/agents/\` for sub-agents
3. Copy \`.env.example\` to \`.env\` and add your Nebius API Key.
4. \`npm run dev\` to start the development server.
5. Last Step: configure the pipeline_end_step.js to handle the final outputs as per your usecase.

## Architecture

This project contains an autonomous agent crew orchestrated by a **MasterAgent**.

### Agents
- **MasterAgent**: Orchestrates the pipeline.
- **Sub-agents**: Specialized workers defined in \`src/agents/\`.
`;

export const NEBIUS_SERVICE = `import * as dotenv from "dotenv";
dotenv.config();

export class NebiusService {
  constructor() {
    this.apiKey = process.env.NEBIUS_API_KEY || "";
    this.baseURL = process.env.NEBIUS_BASE_URL || "https://api.studio.nebius.ai/v1/";
    this.model = process.env.NEBIUS_MODEL || "meta-llama/Meta-Llama-3.1-70B-Instruct";
    if (!this.apiKey) console.warn("Nebius API key missing");
  }
    
  /**
   * General text generation method compatible with the existing agent calls.
   * @param {string} prompt The user prompt
   * @param {object} options Optional settings (systemPrompt, temperature, maxTokens)
   * @returns {Promise<string>} The generated text
   */

  async generateText(prompt, options = {}) {
    const systemContent = options.systemPrompt || "You are a helpful AI assistant. Answer based on the user's input.";
    try {
      const response = await fetch(\`\${this.baseURL}chat/completions\`, {
        method: "POST",
        headers: {
          Authorization: \`Bearer \${this.apiKey}\`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: this.model,
          messages: [
            { role: "system", content: systemContent },
            { role: "user", content: prompt },
          ],
          temperature: options.temperature || 0.7,
        }),
      });

      if (!response.ok) throw new Error(await response.text());
      const result = await response.json();
      return result.choices[0].message?.content || "";
    } catch (error) {
      console.error("Nebius API Error:", error);
      throw error;
    }
  }
}
`;

export const MASTER_AGENT = (projectName, masterName, masterRole, subAgents) => {
  const subAgentNames = subAgents.map(a => a.name);
  
  // Create subscription lists
  const doneEvents = subAgentNames.map(name => `"${name}.done"`).join(",\n    ");
  const startEvents = subAgentNames.map(name => `"${name}.start"`).join(",\n    ");
  
  // Create the master prompt listing available agents
  const agentsList = subAgents.map((a, i) => `${i + 1}. ${a.name} - ${a.description}`).join("\\n");
  const agentsNameList = subAgents.map((a, i) => `${a.name}`).join(", ");

  return `import { NebiusService } from "../service/nebius.service.js";

const MASTER_PROMPT = \`
You are a master orchestrator for the usecase: "${projectName}".
Your role:
${masterRole}

You have following Available Agents:
${agentsList}

Rules:
- You will be provided with the outputs from previous agents, initially this can be empty.
- If the output of the last executed agent is not useful, you can choose to run it again.
- Look at the userinput + agent outputs.
- Decide which agent should run next.
- Say FINISH when the pipeline is complete.

Output must be exactly one string in the following options:
{$agentsNameList, FINISH}
\`;

async function queryPlanner(llm, outputs, logger) {
  const prompt = \`\${MASTER_PROMPT}

Outputs so far:
\${JSON.stringify(outputs, null, 2)}

Respond ONLY with an agent name or FINISH.:\`;
  
  logger?.info("MasterAgent: Planning next step...");
  const text = await llm.generateText(prompt);
  return text.trim();
}

export const config = {
  name: "${masterName}",
  type: "event",
  subscribes: [
    "pipeline.start",
    ${doneEvents}
  ],
  emits: [
    "pipeline.end",
    ${startEvents}
  ], 
  flows: ["${projectName}-flow"]
};

export const handler = async (input, ctx) => {
  
  try {
    const state = await ctx.state.get("pipeline", "active");
    if (!state) {
        ctx.logger.error("state not found for pipeline active");
    } 
    else if (state?.finished) {
        ctx.logger.info("pipeline already finished, ignoring event");
        return;
    }
  } catch (e) {}


  const topic = input.topic;
  const nebiusLlm = new NebiusService();
  
  // --- 1. PIPELINE START ---
  if (topic === "pipeline.start") {
    // TODO: take user inputs out from here; example: const document = input.document || "";
    
    // Init State
    const initialState =  {
      outputs: [{ agent: "user", output:  }], //TODO: pass the user inputs here to add in context; example: { agent: "user", output: document }
      steps: [],
      finished: false
    };
    await ctx.state.set("pipeline", "active", initialState);
    ctx.logger.info("MasterAgent: Event received event = " + JSON.stringify(input));

    // Plan First Step
    const state = await ctx.state.get("pipeline", "active");
    const nextAgent = await queryPlanner(nebiusLlm, state.outputs, ctx.logger);
    
    if (nextAgent === "FINISH") {
       let state = await ctx.state.get("pipeline", "active");
       state.finished = true;
       state.outputs = [...state.outputs, { agent: "master", output: "No agents were run as per planning." }];
       await ctx.state.set("pipeline", "active", state);
       return ctx.emit({ topic: "pipeline.end", data: { outputs: state.outputs } });
    }

    ctx.logger.info(\`MasterAgent: Starting \${nextAgent}\`);
    return ctx.emit({
      topic: \`\${nextAgent}.start\`,
      data: { context: initialState.outputs, topic: \`\${nextAgent}.start\` }
    });
  }

  // --- 2. SUB-AGENT DONE ---
  // Retrieve State
  const pipeline = (await ctx.state.get("pipeline", "active")) || {};
  if (pipeline.finished) return;

  const agentName = topic.replace(".done", "");
  const currentOutputs = [...(pipeline.outputs || []), { agent: agentName, output: input.output }];
  
  // Update State
  pipeline.outputs = currentOutputs;
  await ctx.state.set("pipeline", "active", pipeline);

  // Plan Next Step
  const nextAgent = await queryPlanner(nebiusLlm, currentOutputs, ctx.logger);
  
  if (nextAgent === "FINISH") {
    pipeline.finished = true;
    pipeline.outputs = [...pipeline.outputs, { agent: "master", output: "Pipeline finished as per planning." }];
    await ctx.state.set("pipeline", "active", pipeline);
    
    ctx.logger.info("MasterAgent: Pipeline Finished");
    return ctx.emit({
      topic: "pipeline.end",
      data: { outputs: currentOutputs }
    });
  }

  // Emit Next Agent
  ctx.logger.info(\`MasterAgent: Scheduling \${nextAgent}\`);
  return ctx.emit({
    topic: \`\${nextAgent}.start\`,
    data: { 
      context: currentOutputs, 
      topic: \`\${nextAgent}.start\` 
    }
  });
};
`;
};

export const SUB_AGENT = (projectName,agent) => {
  return `import { PromptTemplate } from "@langchain/core/prompts";
import { NebiusService } from "../service/nebius.service.js";

export const config = {
  name: "${agent.name}",
  type: "event",
  subscribes: ["${agent.name}.start"],
  emits: ["${agent.name}.done"], 
  flows: ["${projectName}-flow"]
};

export const handler = async (input, ctx) => {
  try {
    ctx.logger.info("${agent.name}: processing...");
    
    const state = await ctx.state.get("pipeline", "active");
    const context = state.outputs || input.context || [];      // This is the context from previous agents
    const llmService = new NebiusService();

    //TODO: Customize the prompt and processing logic as per your agent's role; here is a basic example using the dynamic prompt and calling the LLM service.
    let result = "";        // the result of the agents is stored here, dont change the key name
    
    // Dynamic Prompt from Generator
    const agentDefinition = \`${agent.prompt}\`;
    
    // const promptTemplate = PromptTemplate.fromTemplate(
    //   \`You are part of a crew of agents, made for task:${projectName}. \${agentDefinition}\\n\\nContext so far:\\n{context}\\n\\nYour Output:\`
    // );

    // const prompt = await promptTemplate.format({ context: JSON.stringify(context) });
    // result = await llmService.generateText(prompt);

    await ctx.emit({
      topic: "${agent.name}.done",
      data: { output: result, topic: "${agent.name}.done" }
    });

  } catch (e) {
    ctx.logger.error(\`${agent.name} failed: \${e.message}\`);
    await ctx.emit({
      topic: "${agent.name}.done",
      data: { output: "Error in processing", topic: "${agent.name}.done" }
    });
  }
};
`;
};

// Simple start endpoint for the generated project
export const START_PIPELINE_STEP = (projectName) => `export const config = {
  name: "StartPipelineAPI",
  type: "api",
  method: "POST",
  path: "/start",
  emits: ["pipeline.start"], 
  flows: ["${projectName}-flow"]
};

export const handler = async (req, ctx) => {
  const { input } = req.body || {};         // TODO: take out user inputs from here
  await ctx.emit({
    topic: "pipeline.start",                // mandatory field do not remove
    data: { 
      topic: "pipeline.start",              // mandatory field do not remove
                                            // TODO: add the user input here, e.g., input.userQuestion
    }
  });
  return { status: 200, body: { message: "Pipeline started" } };
};
`;


export const END_PIPELINE_STEP  = (projectName) => `export const config = {
  name: "PipelineEndHandler",
  type: "event",
  subscribes: ["pipeline.end"],
  description: "Final step that consumes the pipeline completion event",
  emits: [],
  flows: ["${projectName}-flow"]
};

export const handler = async (event, ctx) => {
  // The MasterAgent emits the final outputs in the event data
  const outputsArray = event.outputs || [];
  const outputs = {};
  for (const o of outputsArray) {
    outputs[o.agent] = o.output;
  }
  
  try {
    ctx.logger.info("Pipeline completed successfully", {
      output_keys: Object.keys(outputs)
    });
    
    // Log details about the specific artifacts generated
    if (outputs.summarizer_agent) {
      ctx.logger.info("Final Summary generated", { 
        preview: outputs.summarizer_agent.substring(0, 50) + "..." 
      });
    }
    
    if (outputs.question_agent) {
      const count = Array.isArray(outputs.question_agent) ? outputs.question_agent.length : 0;
      ctx.logger.info("Final Questions generated", { count });
    }

    // Example: You could add logic here to save these results to a database
    // or send a notification email.

  } catch (e) {
    ctx.logger.error(\`Error in PipelineEndHandler: \${e.message}\`);
  }
};`;